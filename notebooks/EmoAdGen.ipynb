{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c47db-a6f0-48e0-86f1-5f06660c02c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "inference_modifier = {'max_tokens_to_sample':4096, \n",
    "                      \"temperature\":0.5,\n",
    "                      \"top_k\":250,\n",
    "                      \"top_p\":1,\n",
    "                      \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                     }\n",
    "\n",
    "textgen_llm = Bedrock(model_id = \"anthropic.claude-v2\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1044f71-9f3d-4d8d-a226-1bbf43530896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template that has multiple input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"customerServiceManager\", \"customerName\", \"feedbackFromCustomer\"], \n",
    "    template=\"\"\"\n",
    "\n",
    "Human: Create an apology email from the Service Manager {customerServiceManager} to {customerName} in response to the following feedback that was received from the customer: \n",
    "<customer_feedback>\n",
    "{feedbackFromCustomer}\n",
    "</customer_feedback>\n",
    "\n",
    "Assistant:\"\"\"\n",
    ")\n",
    "\n",
    "# Pass in values to the input variables\n",
    "prompt = multi_var_prompt.format(customerServiceManager=\"Bob\", \n",
    "                                 customerName=\"John Doe\", \n",
    "                                 feedbackFromCustomer=\"\"\"Hello Bob,\n",
    "     I am very disappointed with the recent experience I had when I called your customer support.\n",
    "     I was expecting an immediate call back but it took three days for us to get a call back.\n",
    "     The first suggestion to fix the problem was incorrect. Ultimately the problem was fixed after three days.\n",
    "     We are very unhappy with the response provided and may consider taking our business elsewhere.\n",
    "     \"\"\"\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7130b5f1-2b4d-465d-9865-ac6764a6ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = textgen_llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da64ecb-0f86-40e6-b4c8-aaba0522f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = textgen_llm(prompt)\n",
    "\n",
    "email = response[response.index('\\n')+1:]\n",
    "\n",
    "print_ww(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719af45-774a-48eb-9767-73dfbfb068e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weather_description = \"\"\"\\\n",
    "<tool_description>\n",
    "<tool_name>get_weather</tool_name>\n",
    "<parameters>\n",
    "<name>latitude</name>\n",
    "<name>longitude</name>\n",
    "</parameters>\n",
    "</tool_description>\n",
    "\"\"\"\n",
    "\n",
    "get_lat_long_description = \"\"\"\n",
    "<tool_description>\n",
    "<tool_name>get_lat_long</tool_name>\n",
    "<parameters>\n",
    "<name>place</name>  \n",
    "</parameters>\n",
    "</tool_description>\"\"\"\n",
    "\n",
    "list_of_tools_specs = [get_weather_description, get_lat_long_description]\n",
    "tools_string = ''.join(list_of_tools_specs)\n",
    "print(tools_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5d509-31a3-4b14-82e8-e4418384a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "TOOL_TEMPLATE = \"\"\"\\\n",
    "Your job is to formulate a solution to a given <user-request> based on the instructions and tools below.\n",
    "\n",
    "Use these Instructions: \n",
    "1. In this environment you have access to a set of tools and functions you can use to answer the question.\n",
    "2. You can call the functions by using the <function_calls> format below.\n",
    "3. Only invoke one function at a time and wait for the results before invoking another function.\n",
    "4. The Results of the function will be in xml tag <function_results>. Never make these up. The values will be provided for you.\n",
    "5. Only use the information in the <function_results> to answer the question.\n",
    "6. Once you truly know the answer to the question, place the answer in <answer></answer> tags. Make sure to answer in a full sentence which is friendly.\n",
    "7. Never ask any questions\n",
    "\n",
    "<function_calls>\n",
    "<invoke>\n",
    "<tool_name>$TOOL_NAME</tool_name>\n",
    "<parameters>\n",
    "<$PARAMETER_NAME>$PARAMETER_VALUE</$PARAMETER_NAME>\n",
    "...\n",
    "</parameters>\n",
    "</invoke>\n",
    "</function_calls>\n",
    "\n",
    "Here are the tools available:\n",
    "<tools>\n",
    "{tools_string}\n",
    "</tools>\n",
    "\n",
    "<user-request>\n",
    "{user_input}\n",
    "</user-request>\n",
    "\n",
    "Human: What is the first step in order to solve this problem?\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "TOOL_PROMPT = PromptTemplate.from_template(TOOL_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e785f-0619-4604-93a9-2a760448c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import json\n",
    "\n",
    "def invoke_model(prompt):\n",
    "    client = boto3.client(service_name='bedrock-runtime', region_name=os.environ.get(\"AWS_REGION\"),)\n",
    "    body = json.dumps({\"prompt\": prompt, \"max_tokens_to_sample\": 500, \"temperature\": 0,})\n",
    "    modelId = \"anthropic.claude-v2\"\n",
    "    #modelId = \"anthropic.claude-instant-v1\"\n",
    "    response = client.invoke_model(\n",
    "        body=body, modelId=modelId, accept=\"application/json\", contentType=\"application/json\"\n",
    "    )\n",
    "    return json.loads(response.get(\"body\").read()).get(\"completion\")\n",
    "\n",
    "def single_agent_step(prompt, output):\n",
    "\n",
    "    # first check if the model has answered the question\n",
    "    done = False\n",
    "    if '<answer>' in output:\n",
    "        answer = output.split('<answer>')[1]\n",
    "        answer = answer.split('</answer>')[0]\n",
    "        done = True\n",
    "        return done, answer\n",
    "    \n",
    "    # if the model has not answered the question, go execute a function\n",
    "    else:\n",
    "\n",
    "        # parse the output for any \n",
    "        function_xml = output.split('<function_calls>')[1]\n",
    "        function_xml = function_xml.split('</function_calls>')[0]\n",
    "        function_dict = xmltodict.parse(function_xml)\n",
    "        func_name = function_dict['invoke']['tool_name']\n",
    "        parameters = function_dict['invoke']['parameters']\n",
    "\n",
    "        #print(f\"single_agent_step:: func_name={func_name}::params={parameters}::function_dict={function_dict}::\")\n",
    "        # call the function which was parsed\n",
    "        func_response = call_function(func_name, parameters)\n",
    "\n",
    "        # create the next human input\n",
    "        func_response_str = '\\n\\nHuman: Here is the result from your function call\\n\\n'\n",
    "        func_response_str = func_response_str + f'<function_results>\\n{func_response}\\n</function_results>'\n",
    "        func_response_str = func_response_str + '\\n\\nIf you know the answer, say it. If not, what is the next step?\\n\\nAssistant:'\n",
    "\n",
    "        # augment the prompt\n",
    "        prompt = prompt + output + func_response_str\n",
    "    return done, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1b603-1d16-416e-bf8c-cc1ca880aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'What is the weather in Las Vegas?'\n",
    "next_step = TOOL_PROMPT.format(tools_string=tools_string, user_input=user_input)\n",
    "\n",
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_agent_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d050339b-7f16-4483-9bfe-038d5e88b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_agent_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7caaedf-ef24-4e1c-bb29-ce77f80601c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import ast\n",
    "\n",
    "@tool (\"get_lat_long\")\n",
    "def get_lat_long(place: str) -> dict:\n",
    "    \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    response = requests.get(url, params=params).json()\n",
    "\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "@tool (\"get_weather_dict\")\n",
    "def get_weather_dict(co_ord_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns weather data for a given a dictionary having latitude and longitude.\n",
    "    \"\"\"\n",
    "    if '{' in co_ord_str:\n",
    "        co_ord_str = ast.literal_eval(co_ord_str)\n",
    "        latitude = co_ord_str['latitude']\n",
    "        longitude = co_ord_str['longitude']\n",
    "    else:\n",
    "        latitude = float(co_ord_str.split(\",\")[0].strip())\n",
    "        longitude = float(co_ord_str.split(\",\")[1].strip())\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1ffed-0b9d-4130-aa19-2253a506fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "math_tool_template = \"\"\"\\\n",
    "Given a question from the Human with a math problem, provide only a single line mathematical expression that solves the problem in the following format.\n",
    "Never solve the expression only create a parsable expression.\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "\n",
    "Here is an example response with a single line mathematical expression for solving a math problem:\n",
    "```text\n",
    "37593**(1/5)\n",
    "```\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=agent_llm, verbose=True)\n",
    "llm_math_chain.llm_chain.prompt.template = math_tool_template\n",
    "result = llm_math_chain('What is nine times 465?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3014a-7cdb-4938-a261-0acfc8b6ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "math_tool_template = \"\"\"\\\n",
    "Given a question from the Human with a math problem, provide only a single line mathematical expression that solves the problem in the following format.\n",
    "Never solve the expression only create a parsable expression.\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "\n",
    "Here is an example response with a single line mathematical expression for solving a math problem:\n",
    "```text\n",
    "37593**(1/5)\n",
    "```\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=agent_llm, verbose=True)\n",
    "llm_math_chain.llm_chain.prompt.template = math_tool_template\n",
    "result = llm_math_chain('What is nine times 465?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b3a1a-c771-4835-945a-146b7f0ebfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of [ \"get_lat_long\", \"get_weather_dict\", \"Calculator\"]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "react_agent.agent.llm_chain.prompt.template=prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d14ebf-ac53-48d3-b42c-3b3d97251654",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = react_agent(\"Can you check the weather in Las Vegas for me?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
